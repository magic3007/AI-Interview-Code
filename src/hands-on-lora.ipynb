{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, lora_alpha):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.scale = lora_alpha / rank\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features) # Y = X * W^T + b\n",
    "        # size of X: (batch_size, in_features)\n",
    "        # size of self.linear.weight (i.e., W): (out_features, in_features)\n",
    "\n",
    "        # Y = X * (W + A * B)^T + b\n",
    "        # size of A: (out_features, rank)\n",
    "        # size of B: (rank, in_features)\n",
    "\n",
    "        if self.rank > 0:\n",
    "            # 这里需要nn.Parameter, 这样model.parameters()才会包含这个参数\n",
    "            self.lora_a = nn.Parameter(torch.randn(out_features, rank))\n",
    "            nn.init.kaiming_normal_(self.lora_a, a=0.01)\n",
    "            self.lora_b = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        else:\n",
    "            self.lora_a = None\n",
    "            self.lora_b = None\n",
    "\n",
    "        # 设置linear的参数为不可训练\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.rank > 0:\n",
    "            # Y = X * (W + A * B)^T + b = X * W^T + b + X * (A * B)^T\n",
    "            return self.linear(X) + self.scale * X @ (self.lora_a @ self.lora_b).T\n",
    "        else:\n",
    "            return self.linear(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Shape of Y: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Shape of Y: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m20\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">s: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.559602737426758</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "s: \u001b[1;36m11.559602737426758\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">grad sum of lora.lora_a:  <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>.<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "grad sum of lora.lora_a:  \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m.\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">grad sum of lora.lora_b:  <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">43.8912</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "grad sum of lora.lora_b:  \u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m43.8912\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">grad of lora.linear.bias:  <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "grad of lora.linear.bias:  \u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">grad of lora.linear.weight:  <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "grad of lora.linear.weight:  \u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "in_features = 10\n",
    "out_features = 20\n",
    "rank = 2\n",
    "lora_alpha = 1\n",
    "\n",
    "X = torch.randn(batch_size, in_features)\n",
    "\n",
    "lora = LoRA(in_features, out_features, rank, lora_alpha)\n",
    "\n",
    "Y = lora(X)\n",
    "\n",
    "print(f\"Shape of Y: {Y.shape}\")\n",
    "\n",
    "s = Y.sum()\n",
    "\n",
    "print(f\"s: {s}\")\n",
    "\n",
    "s.backward()\n",
    "\n",
    "print(\"grad sum of lora.lora_a: \", lora.lora_a.grad.sum())\n",
    "print(\"grad sum of lora.lora_b: \", lora.lora_b.grad.sum())\n",
    "print(\"grad of lora.linear.bias: \", lora.linear.bias.grad)\n",
    "# \\frac{\\partial s}{\\partial A} = B @ X^T\n",
    "print(\"grad of lora.linear.weight: \", lora.linear.weight.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
